{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 픽쳐와 커널은 같은 뜻 ? 그래서 커널과 가중치는 같은 뜻? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22fb16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f2f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Conv2D, AveragePooling2D, Flatten, Dense, ZeroPadding2D\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d5417",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb92aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#  MNIST 데이터셋을 불러옴 \n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74073204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data는 32 * 32 데이터이므로 28 * 28로 reshape 해줌\n",
    "# reshape(batch_size, width, height, channel)\n",
    "# batch_size = -1 : 자동으로 batch_size를 조정\n",
    "# width = 28, height = 28\n",
    "# channel = 1 (흑백), 컬러는 3 \n",
    "\n",
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "x_test = x_test.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d6f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fc50eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8bit의 이미지이므로 (0-255범위로) 나누어줌\n",
    "# 8bit = 2의 8승 = 256가지의 경우의 수를 표현할 수 있는 단위임 \n",
    "# 그래서 0부터 255범위로 나누어주는 것이므로 255로 나눔 \n",
    "\n",
    "x_train=x_train/255.0\n",
    "x_val = x_val/255.0\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a657ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet = Sequential([InputLayer(input_shape=(28,28,1)), \n",
    "                    ZeroPadding2D((2,2)),\n",
    "                    Conv2D(6,5, activation=\"tanh\"),\n",
    "                    AveragePooling2D(strides=2),\n",
    "                    Conv2D(16,5,activation=\"tanh\"),\n",
    "                    AveragePooling2D(strides=2),\n",
    "                    Conv2D(120,5, activation=\"tanh\"),\n",
    "                    Flatten(),\n",
    "                    Dense(84,activation=\"tanh\"),\n",
    "                    Dense(10,activation=\"softmax\")])\n",
    "\n",
    "# 이렇게 리스트 형태로 넣는 방법도 있고\n",
    "# add 형식으로 추가해가는 방법도 있다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a55cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3개의 Convolution Layer (C1, C3, C5)\n",
    "# 2개의 Pooling layer (S2, S4)\n",
    "# 1개의 FC Layer (F6)로 구성\n",
    "# 활성화 함수로는 tanh 사용\n",
    "\n",
    "LeNet = Sequential([InputLayer(input_shape=(28,28,1)), \n",
    "                    # 입력 레이어\n",
    "                    \n",
    "                    ZeroPadding2D((2,2)),\n",
    "                    # 패딩\n",
    "                    # ZeroPadding = 패딩을 0으로 채워준다 \n",
    "                    # padding = 2, 2\n",
    "                    # 2칸씩 늘리니까 총 4칸이 늘어난다 \n",
    "                    \n",
    "                    Conv2D(6,5, activation=\"tanh\"),\n",
    "                    \n",
    "                    AveragePooling2D(strides=2),\n",
    "                    # strides: 필터를 적용하는 위치 간격\n",
    "                    \n",
    "                    Conv2D(16,5,activation=\"tanh\"),\n",
    "                    \n",
    "                    AveragePooling2D(strides=2),\n",
    "                    \n",
    "                    Conv2D(120,5, activation=\"tanh\"),\n",
    "                    \n",
    "                    Flatten(),\n",
    "                    # Flatten() 1차원 배열로 변환 (평탄화)\n",
    "                    # 리니어 리그레이션에 들어갈 수 있도록 입체적인것을 평탄하게 만드는 작업 \n",
    "                    \n",
    "                    Dense(84,activation=\"tanh\"),\n",
    "                    \n",
    "                    Dense(10,activation=\"softmax\")])\n",
    "                    # 0부터 9까지니까 총 10개의 아웃풋을 생성 \n",
    "                    # softmax는 합이 1이 되도록 출력하는데 이건 분류모델이기에 합이 1이라는건 확률값과 동일\n",
    "                    # 분류모델에서 3개이상이면 softmax를 사용하고\n",
    "                    # 2개면 바이너리를 사용함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4890e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet.compile(optimizer=\"SGD\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a2219a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " zero_padding2d (ZeroPaddin  (None, 32, 32, 1)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 6)         156       \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 14, 14, 6)         0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 5, 5, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 1, 120)         48120     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 120)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61706 (241.04 KB)\n",
      "Trainable params: 61706 (241.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LeNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a5f67e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.7057 - accuracy: 0.8291\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.2909 - accuracy: 0.9152\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.2267 - accuracy: 0.9331\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.1839 - accuracy: 0.9452\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.1525 - accuracy: 0.9550\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.1301 - accuracy: 0.9617\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.1128 - accuracy: 0.9662\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.0997 - accuracy: 0.9709\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.0896 - accuracy: 0.9738\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.0811 - accuracy: 0.9761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2815c060bb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LeNet.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da00a21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0739 - accuracy: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07394474744796753, 0.9768999814987183]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LeNet.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad1bffc",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a86a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet = Sequential([InputLayer(input_shape=(28,28,1)), \n",
    "#                     ZeroPadding2D((2,2)),\n",
    "#                     Conv2D(6,5, activation=\"tanh\"),\n",
    "#                     AveragePooling2D(strides=2),\n",
    "#                     Conv2D(16,5,activation=\"tanh\"),\n",
    "#                     AveragePooling2D(strides=2),\n",
    "#                     Conv2D(120,5, activation=\"tanh\"),\n",
    "#                     Flatten(),\n",
    "#                     Dense(84,activation=\"tanh\"),\n",
    "#                     Dense(10,activation=\"softmax\")])\n",
    "\n",
    "# 이렇게 리스트 형태로 넣는 방법도 있고\n",
    "# add 형식으로 추가해가는 방법도 있다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f55b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name='alex_net')\n",
    "model.add(Conv2D(96, 11, strides=4))\n",
    "model.add(AveragePooling2D(strides=2))\n",
    "model.add(Conv2D(256, 5))\n",
    "model.add(AveragePooling2D(strides=2))\n",
    "model.add(Conv2D(384, 3))\n",
    "model.add(Conv2D(384, 3))\n",
    "model.add(Conv2D(256, 3))\n",
    "model.add(AveragePooling2D(strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff0b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation는 자유롭게 사용하면 된다. \n",
    "# 현재 relu를 사용한 이유는 해당 대회에서 우승한 사람이 선택한 것 \n",
    "\n",
    "model = Sequential(name='alex_net')\n",
    "model.add(Conv2D(filters = 96,\n",
    "                 kernel_size = 11,\n",
    "                 strides = 4,\n",
    "                 activation = 'relu',\n",
    "                 input_shape = (227, 227, 3)\n",
    "                ))\n",
    "model.add(MaxPool2D(pool_size = 3,\n",
    "                    strides = 2\n",
    "                   ))\n",
    "\n",
    "model.add(Conv2D(filters = 256,\n",
    "                 kernel_size = 5,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "model.add(MaxPool2D(pool_size = 3,\n",
    "                    strides = 2\n",
    "                   ))\n",
    "\n",
    "# 구조에 strides가 안써있으면 기본값이 1이다\n",
    "# padding = 'same'라고 한 이유\n",
    "# 데이터를 확인해보면 padding이 string값이라 숫자를 넣을 수 없다 그래서 그냥 일단 same 한거임 \n",
    "\n",
    "model.add(Conv2D(filters = 384,\n",
    "                 kernel_size = 3,\n",
    "                 strides=1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "model.add(Conv2D(filters = 384,\n",
    "                 kernel_size = 3,\n",
    "                 strides=1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "model.add(Conv2D(filters = 256,\n",
    "                 kernel_size = 3,\n",
    "                 strides=1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "model.add(MaxPool2D(pool_size = 3,\n",
    "                    strides = 2\n",
    "                   ))\n",
    "\n",
    "# Dense 2번 넣은 이유\n",
    "# LeNet을 보완해서 alex_net을 만들었는데 \n",
    "# 그들이 실험을 해보았는데 2번을 넣었더니 잘 되어서 2번을 넣은 것\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='sortmax'))\n",
    "# num_classes\n",
    "# 몇개로 출력하는 것이 아니므로 \n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"]\n",
    "             )\n",
    "\n",
    "# model.compile\n",
    "# optimizer 정규화기\n",
    "## 훈련과정을 설정\n",
    "## adam\n",
    "## sgd\n",
    "## rmsprop\n",
    "## adagrad 등\n",
    "\n",
    "# loss 손실함수\n",
    "## mse\n",
    "## categorical_crossentropy\n",
    "## binary_crossentropy\n",
    "\n",
    "# metrics 평가지표\n",
    "## 분류: accuracy\n",
    "## 회귀: mse, rmse, r2, mae, mspe, mape, msle 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential(name='alex_net')\n",
    "# model.add(Conv2D(filters=96, kernel_size=11,strides=4,activation='relu',input_shape=(227,227,3)))\n",
    "# model.add(MaxPool2D(pool_size=3,strides=2))\n",
    "\n",
    "# model.add(Conv2D(filters=256, kernel_size=5, activation='relu',padding=\"same\"))\n",
    "# model.add(MaxPool2D(pool_size=3,strides=2))\n",
    "\n",
    "# model.add(Conv2D(filters=384, kernel_size=3,strides=1,activation='relu',padding=\"same\"))\n",
    "# model.add(Conv2D(filters=384,kernel_size=3,strides=1,activation='relu',padding=\"same\"))\n",
    "# model.add(Conv2D(filters=256,kernel_size=3,strides=1,activation='relu',padding=\"same\"))\n",
    "# model.add(MaxPool2D(pool_size=3,strides=2))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(4096,activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(4096,activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da902baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "829358bc",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef40766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bdfd52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=\"./dogs_vs_cats/train\",\n",
    "    #directory=\"./content/train\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode='int',\n",
    "    batch_size=32,\n",
    "    image_size=(227,227)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5475a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=\"./dogs_vs_cats/test\",\n",
    "    labels=\"inferred\", # labels are generated from the directory structure\n",
    "    label_mode='int', # 0=cats & 1=dogs\n",
    "    batch_size=32,\n",
    "    image_size=(227,227)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad34183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize 정규화\n",
    "def process(image, label):\n",
    "    image = tf.cast(image/255., tf.float32)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(process)\n",
    "test_ds = test_ds.map(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6a675f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name='alex_net')\n",
    "model.add(Conv2D(filters=96, kernel_size=11,strides=4,activation='relu',input_shape=(227,227,3)))\n",
    "model.add(MaxPool2D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=5, activation='relu',padding=\"same\"))\n",
    "model.add(MaxPool2D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=384, kernel_size=3,strides=1,activation='relu',padding=\"same\"))\n",
    "model.add(Conv2D(filters=384,kernel_size=3,strides=1,activation='relu',padding=\"same\"))\n",
    "model.add(Conv2D(filters=256,kernel_size=3,strides=1,activation='relu',padding=\"same\"))\n",
    "model.add(MaxPool2D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90f75bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 1436s 2s/step - loss: nan - accuracy: 0.5002 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1407s 2s/step - loss: nan - accuracy: 0.5000 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "425/625 [===================>..........] - ETA: 7:06 - loss: nan - accuracy: 0.5024"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=10,validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b48ae",
   "metadata": {},
   "source": [
    "학습이 더이상 오르지 않는 이유 \n",
    "이유가 다양하다 \n",
    "1. \n",
    "0 또는 1이 들어가야하는데 int가 들어가야하는데 float가 들어간다던가 (혹은 반대)의 경우\n",
    "\n",
    "2. \n",
    "이진분류에서 자주 발생하는 문제인데\n",
    "예측을 0으로만 한다던가 1로만 한다던가 등 \n",
    "\n",
    "3. \n",
    "loss를 binary로 해야하는데 categorical로 했다는 등등\n",
    "\n",
    "다양한 이유가 존재함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0912892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f39fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name = 'vgg16')\n",
    "model.add(Conv2D(filters = 64,\n",
    "                 kernel_size = 3,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same',\n",
    "                 input_shape = (224, 224, 3)\n",
    "                ))\n",
    "model.add(Conv2D(filters = 64,\n",
    "                 kernel_size = 3,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "\n",
    "model.add(MaxPool2D(pool_size = 2,\n",
    "                    strides = 2\n",
    "                   ))\n",
    "model.add(Conv2D(filters = 128,\n",
    "                 kernel_size = 3,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "model.add(Conv2D(filters = 128,\n",
    "                 kernel_size = 3,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "\n",
    "model.add(MaxPool2D(pool_size = 2,\n",
    "                    strides = 2\n",
    "                   ))\n",
    "model.add(Conv2D(filters = 256,\n",
    "                 kernel_size = 3,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "model.add(Conv2D(filters = 256,\n",
    "                 kernel_size = 3,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "model.add(Conv2D(filters = 256,\n",
    "                 kernel_size = 1,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "\n",
    "model.add(MaxPool2D(pool_size = 2,\n",
    "                    strides = 2\n",
    "                   ))\n",
    "model.add(Conv2D(filters = 512,\n",
    "                 kernel_size = 3,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "model.add(Conv2D(filters = 512,\n",
    "                 kernel_size = 3,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "model.add(Conv2D(filters = 512,\n",
    "                 kernel_size = 1,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "\n",
    "model.add(MaxPool2D(pool_size = 2,\n",
    "                    strides = 2\n",
    "                   ))\n",
    "model.add(Conv2D(filters = 512,\n",
    "                 kernel_size = 3,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "model.add(Conv2D(filters = 512,\n",
    "                 kernel_size = 3,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "model.add(Conv2D(filters = 512,\n",
    "                 kernel_size = 1,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding = 'same'\n",
    "                ))\n",
    "\n",
    "model.add(MaxPool2D(pool_size = 2,\n",
    "                    strides = 2\n",
    "                   ))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096,activation='relu'))\n",
    "model.add(Dense(4096,activation='relu'))\n",
    "# 분리해야할 클래스를 넣어주어야한다\n",
    "# 그 개수는 분리해야할 개수를 넣어주어야한다 \n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# 개/고양이 분류에서는 0과 1 이므로 1을 넣어주어야하고, 활성화함수는 시그모이드를 사용해야함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2236e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8643e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4daa5300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 112, 112, 64)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 56, 56, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 56, 56, 256)       65792     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 28, 28, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 28, 28, 512)       262656    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 512)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 14, 14, 512)       262656    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 7, 7, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 129546049 (494.18 MB)\n",
      "Trainable params: 129546049 (494.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b48aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcbcc2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIG2-003-008\\AppData\\Local\\Temp\\ipykernel_5740\\1698968050.py:19: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  train_jpg_path_series = pd.Series(train_jpg_path,name=\"JPG\").astype('str')\n",
      "C:\\Users\\BIG2-003-008\\AppData\\Local\\Temp\\ipykernel_5740\\1698968050.py:20: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  train_jpg_labels_series = pd.Series(train_jpg_labels,name=\"CATEGORY\").astype('str')\n",
      "C:\\Users\\BIG2-003-008\\AppData\\Local\\Temp\\ipykernel_5740\\1698968050.py:22: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  test_jpg_path_series = pd.Series(test_jpg_path,name=\"JPG\").astype(str)\n",
      "C:\\Users\\BIG2-003-008\\AppData\\Local\\Temp\\ipykernel_5740\\1698968050.py:23: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  test_jpg_labels_series = pd.Series(test_jpg_labels,name=\"CATEGORY\").astype(str)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'CATEGORY'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'CATEGORY'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 44\u001b[0m\n\u001b[0;32m     30\u001b[0m train_img_generator \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m,\n\u001b[0;32m     31\u001b[0m                                           rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,\n\u001b[0;32m     32\u001b[0m                                    brightness_range\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m0.7\u001b[39m],\n\u001b[0;32m     33\u001b[0m                                    width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     34\u001b[0m                                    height_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     35\u001b[0m                                    zoom_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     37\u001b[0m test_img_generator \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m,\n\u001b[0;32m     38\u001b[0m                                        rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,\n\u001b[0;32m     39\u001b[0m                                         brightness_range\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m0.7\u001b[39m],\n\u001b[0;32m     40\u001b[0m                                         width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     41\u001b[0m                                         height_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     42\u001b[0m                                         zoom_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_img_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_train_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mx_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJPG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43my_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCATEGORY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m227\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m227\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m test_img_generator\u001b[38;5;241m.\u001b[39mflow_from_dataframe(dataframe\u001b[38;5;241m=\u001b[39mmain_test_data,\n\u001b[0;32m     54\u001b[0m                                                       x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJPG\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     55\u001b[0m                                                       y_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCATEGORY\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m                                                       subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     60\u001b[0m                                                       target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m227\u001b[39m,\u001b[38;5;241m227\u001b[39m))\n\u001b[0;32m     62\u001b[0m train_valid \u001b[38;5;241m=\u001b[39mtrain_img_generator\u001b[38;5;241m.\u001b[39mflow_from_dataframe(dataframe\u001b[38;5;241m=\u001b[39mmain_train_data,\n\u001b[0;32m     63\u001b[0m                                                        x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJPG\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m                                                        y_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCATEGORY\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m                                                        subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     69\u001b[0m                                                           target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m227\u001b[39m,\u001b[38;5;241m227\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\preprocessing\\image.py:1806\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_dataframe\u001b[1;34m(self, dataframe, directory, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, subset, interpolation, validate_filenames, **kwargs)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_duplicates\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m   1800\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_duplicates is deprecated, you can drop duplicates \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1802\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby using the pandas.DataFrame.drop_duplicates method.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1803\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m   1804\u001b[0m     )\n\u001b[1;32m-> 1806\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1811\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1821\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1823\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1824\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1825\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_filenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1828\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\preprocessing\\image.py:974\u001b[0m, in \u001b[0;36mDataFrameIterator.__init__\u001b[1;34m(self, dataframe, directory, image_data_generator, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, subset, interpolation, keep_aspect_ratio, dtype, validate_filenames)\u001b[0m\n\u001b[0;32m    972\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_valid_filepaths(df, x_col)\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_output\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m--> 974\u001b[0m     df, classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(classes)\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# build an index of all the unique classes\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\preprocessing\\image.py:1111\u001b[0m, in \u001b[0;36mDataFrameIterator._filter_classes\u001b[1;34m(df, y_col, classes)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1110\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m-> 1111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_col\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[0;32m   1112\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   1113\u001b[0m             classes\u001b[38;5;241m.\u001b[39mupdate(v)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'CATEGORY'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "train_path = Path(\"/Brain_Tumor_Classification/Training\")\n",
    "test_path = Path(\"/Brain_Tumor_Classification/Testing\")\n",
    "\n",
    "train_jpg_path = list(train_path.glob(r\"*/*.jpg\"))\n",
    "test_jpg_path = list(test_path.glob(r\"*/*.jpg\"))\n",
    "\n",
    "train_jpg_labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],train_jpg_path))\n",
    "test_jpg_labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],test_jpg_path))\n",
    "\n",
    "train_jpg_path_series = pd.Series(train_jpg_path,name=\"JPG\").astype('str')\n",
    "train_jpg_labels_series = pd.Series(train_jpg_labels,name=\"CATEGORY\").astype('str')\n",
    "\n",
    "test_jpg_path_series = pd.Series(test_jpg_path,name=\"JPG\").astype(str)\n",
    "test_jpg_labels_series = pd.Series(test_jpg_labels,name=\"CATEGORY\").astype(str)\n",
    "\n",
    "main_train_data = pd.concat([train_jpg_path_series, train_jpg_labels_series],axis=1)\n",
    "main_test_data = pd.concat([test_jpg_path_series, test_jpg_labels_series],axis=1)\n",
    "main_train_data = main_train_data.sample(frac=1).reset_index(drop=True)\n",
    "main_test_data = main_test_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_img_generator = ImageDataGenerator(rescale=1./255,\n",
    "                                          rotation_range=25,\n",
    "                                   brightness_range=[0.3,0.7],\n",
    "                                   width_shift_range=0,\n",
    "                                   height_shift_range=0,\n",
    "                                   zoom_range=0)\n",
    "\n",
    "test_img_generator = ImageDataGenerator(rescale=1./255,\n",
    "                                       rotation_range=25,\n",
    "                                        brightness_range=[0.3,0.7],\n",
    "                                        width_shift_range=0,\n",
    "                                        height_shift_range=0,\n",
    "                                        zoom_range=0)\n",
    "\n",
    "train_ds = train_img_generator.flow_from_dataframe(dataframe=main_train_data,\n",
    "                                                    x_col=\"JPG\",\n",
    "                                                       y_col=\"CATEGORY\",\n",
    "                                                       color_mode=\"rgb\",\n",
    "                                                       class_mode=\"categorical\",\n",
    "                                                       batch_size=10,\n",
    "                                                       subset=\"training\",\n",
    "                                                       target_size=(227,227))\n",
    "\n",
    "test_ds = test_img_generator.flow_from_dataframe(dataframe=main_test_data,\n",
    "                                                      x_col=\"JPG\",\n",
    "                                                      y_col=\"CATEGORY\",\n",
    "                                                      color_mode='rgb',\n",
    "                                                      class_mode=\"categorical\",\n",
    "                                                      batch_size=10,\n",
    "                                                      subset=\"training\",\n",
    "                                                      target_size=(227,227))\n",
    "\n",
    "train_valid =train_img_generator.flow_from_dataframe(dataframe=main_train_data,\n",
    "                                                       x_col=\"JPG\",\n",
    "                                                       y_col=\"CATEGORY\",\n",
    "                                                       color_mode=\"rgb\",\n",
    "                                                       class_mode=\"categorical\",\n",
    "                                                       batch_size=10,\n",
    "                                                       subset=\"training\",\n",
    "                                                          target_size=(227,227))\n",
    "\n",
    "test_valid =test_img_generator.flow_from_dataframe(dataframe=main_test_data,\n",
    "                                                              x_col=\"JPG\",\n",
    "                                                              y_col=\"CATEGORY\",\n",
    "                                                              color_mode=\"rgb\",\n",
    "                                                              class_mode=\"categorical\",\n",
    "                                                              batch_size=10,\n",
    "                                                              subset=\"training\",\n",
    "                                                              target_size=(227,227))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71d2e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet152():\n",
    "  model = tf.keras.applications.ResNet152(weights='imagenet',include_top = False, input_shape=(224,224,3))\n",
    "  \n",
    "  x = Flatten()(model.output)\n",
    "  x = Dense(1000, activation='relu')(x)\n",
    "  predictions = Dense(4,activation='softmax')(x)\n",
    "\n",
    "  head_model = Model(inputs = model.input, outputs = predictions)\n",
    "  head_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "  return head_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c04180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5ee86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffd8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1028fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366fd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fc729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d200f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d6600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2581b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94b076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e09c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946b72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0443cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93e78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db96239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c52592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679d579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec867653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eba5af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec294412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5befb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d997fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b2a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eccafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b5eebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7766eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
